{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import platform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "input_file = \"Eurovision before 2016.csv\"\n",
    "test_file = \"eurovision 2016.csv\"\n",
    "test_file2 = \"eurovision 2016-2.csv\"\n",
    "points_per_country = 1+2+3+4+5+6+7+8+10+12\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_file)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "data['Normalized Score'] = data['Score']/(data['Countries Voting'].apply(float))\n",
    "data['Normalized Score 2'] = data['Normalized Score'] - points_per_country/(data['Countries competing'].apply(float))\n",
    "data['Country'] = data['Country'].apply(str.strip)\n",
    "data['Normalized YT count'] = 1.0/data['Countries competing']\n",
    "data.loc[data['Videocounts']!=0,'Normalized YT count'] = data.loc[data['Videocounts']!=0,'Videocounts'].apply(float)/data.loc[data['Videocounts']!=0,'Total Videocounts for round']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Normalized Score 2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_file2)\n",
    "test_data.fillna(0,inplace=True)\n",
    "\n",
    "test_data['Normalized Score'] = test_data['Score']/(data['Countries Voting'].apply(float))\n",
    "test_data['Normalized Score 2'] = test_data['Normalized Score'] - points_per_country/(test_data['Countries competing'].apply(float))\n",
    "test_data['Country'] = test_data['Country'].apply(str.strip)\n",
    "test_data['Normalized YT count'] = 1.0/test_data['Countries competing']\n",
    "test_data.loc[test_data['Videocounts']!=0,'Normalized YT count'] = test_data.loc[test_data['Videocounts']!=0,'Videocounts'].apply(float)/test_data.loc[test_data['Videocounts']!=0,'Total Videocounts for round']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data['Country']=='Malta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawcount = [0]*27\n",
    "for i in range(1,27):\n",
    "    data_set = data[(data['Draw']==i) ]['Normalized Score']*24\n",
    "    print i, data_set.mean(),  data_set.std(), data_set.count()\n",
    "    drawcount[i] = drawcount[i-1] + data_set.mean()\n",
    "print(drawcount)\n",
    "print(data['Normalized Score'].mean()*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Draw','Male singer','Female Singer','Duo','Band','Experienced artist','Final','Former Soviet','Baltic','Caucasus','Scandinavia',\n",
    "            'Tiny Country','Former Yugoslavia','Big Five','Nordic','Orthodox','Mediterranean','East Europe','British Isles',\n",
    "            'German Speaking','French Speaking','Northwest Europe','Romance Language','Germanic Language','Slavic Language','Normalized YT count']\n",
    "small_features = ['Draw','Male singer','Female Singer','Duo','Band','Experienced artist','Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_features = ['Normalized Score 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "former_soviet = ['Russia','Belarus','Ukraine','Latvia','Lithuania','Estonia','Georgia','Moldova','Armenia','Azerbaijan']\n",
    "baltic = ['Latvia','Lithuania','Estonia']\n",
    "caucasus = ['Armenia','Azerbaijan','Georgia']\n",
    "scandinavia =['Denmark','Sweden','Norway']\n",
    "tiny_country = ['San Marino','Monaco','Andorra']\n",
    "former_yugoslavia = ['Serbia','Montenegro','Slovenia','Croatia','Macedonia','Bosnia and Herzegovina']\n",
    "big_five = ['United Kingdom','Germany','France','Italy','Spain']\n",
    "nordic = ['Denmark','Sweden','Norway','Iceland','Finland']\n",
    "orthodox = ['Greece','Serbia','Bulgaria','Ukraine','Belarus','Russia','Romania','Moldova','Georgia','Armenia','Montenegro']\n",
    "mediterranean = ['France','Spain','Italy','Monaco','Malta','Albania','Croatia','Greece','Turkey','Cyprus','Israel','Portugal']\n",
    "east_europe = ['Poland','Czech Republic','Slovakia','Hungary','Bulgaria','Romania']\n",
    "british_isles = ['United Kingdom','Ireland']\n",
    "german_speaking = ['Germany','Austria','Switzerland']\n",
    "french_speaking = ['France','Switzerland','Belgium']\n",
    "north_west_europe = ['United Kingdom','Belgium','Netherlands','Ireland','Germany']\n",
    "romance_language = ['Portugal','Spain','Andorra','Belgium','France','Switzerland','Italy','San Marino','Romania','Moldova']\n",
    "germanic_language = ['United Kingdom','Ireland','Netherlands','Belgium','Germany','Switzerland','Denmark','Sweden','Iceland','Norway']\n",
    "slavic_language = ['Poland','Czech Republic','Slovakia','Bulgaria','Belarus','Ukraine','Serbia','Croatia','Slovenia','Macedonia','Montenegro','Bosnia and Herzegovina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Former Soviet'] = 0\n",
    "data.loc[data['Country'].isin(former_soviet),'Former Soviet'] = 1\n",
    "data['Baltic'] = 0\n",
    "data.loc[data['Country'].isin(baltic),'Baltic'] = 1\n",
    "data['Caucasus'] = 0\n",
    "data.loc[data['Country'].isin(caucasus),'Caucasus'] = 1\n",
    "data['Scandinavia'] = 0\n",
    "data.loc[data['Country'].isin(scandinavia),'Scandinavia'] = 1\n",
    "data['Tiny Country'] = 0\n",
    "data.loc[data['Country'].isin(tiny_country),'Tiny Country'] = 1\n",
    "data['Former Yugoslavia'] = 0\n",
    "data.loc[data['Country'].isin(former_yugoslavia),'Former Yugoslavia'] = 1\n",
    "data['Big Five'] = 0\n",
    "data.loc[data['Country'].isin(big_five),'Big Five'] = 1\n",
    "data['Nordic'] = 0\n",
    "data.loc[data['Country'].isin(nordic),'Nordic'] = 1\n",
    "data['Orthodox'] = 0\n",
    "data.loc[data['Country'].isin(orthodox),'Orthodox'] = 1\n",
    "data['Mediterranean'] = 0\n",
    "data.loc[data['Country'].isin(mediterranean),'Mediterranean'] = 1\n",
    "data['East Europe'] = 0\n",
    "data.loc[data['Country'].isin(east_europe),'East Europe'] = 1\n",
    "data['British Isles'] = 0\n",
    "data.loc[data['Country'].isin(british_isles),'British Isles'] = 1\n",
    "data['German Speaking'] = 0\n",
    "data.loc[data['Country'].isin(german_speaking),'German Speaking'] = 1\n",
    "data['French Speaking'] = 0\n",
    "data.loc[data['Country'].isin(french_speaking),'French Speaking'] = 1\n",
    "data['Northwest Europe'] = 0\n",
    "data.loc[data['Country'].isin(north_west_europe),'Northwest Europe'] = 1\n",
    "data['Romance Language'] = 0\n",
    "data.loc[data['Country'].isin(romance_language),'Romance Language'] = 1\n",
    "data['Germanic Language'] = 0\n",
    "data.loc[data['Country'].isin(germanic_language),'Germanic Language'] = 1\n",
    "data['Slavic Language'] = 0\n",
    "data.loc[data['Country'].isin(slavic_language),'Slavic Language'] = 1\n",
    "\n",
    "test_data['Former Soviet'] = 0\n",
    "test_data.loc[test_data['Country'].isin(former_soviet),'Former Soviet'] = 1\n",
    "test_data['Baltic'] = 0\n",
    "test_data.loc[test_data['Country'].isin(baltic),'Baltic'] = 1\n",
    "test_data['Caucasus'] = 0\n",
    "test_data.loc[test_data['Country'].isin(caucasus),'Caucasus'] = 1\n",
    "test_data['Scandinavia'] = 0\n",
    "test_data.loc[test_data['Country'].isin(scandinavia),'Scandinavia'] = 1\n",
    "test_data['Tiny Country'] = 0\n",
    "test_data.loc[test_data['Country'].isin(tiny_country),'Tiny Country'] = 1\n",
    "test_data['Former Yugoslavia'] = 0\n",
    "test_data.loc[test_data['Country'].isin(former_yugoslavia),'Former Yugoslavia'] = 1\n",
    "test_data['Big Five'] = 0\n",
    "test_data.loc[test_data['Country'].isin(big_five),'Big Five'] = 1\n",
    "test_data['Nordic'] = 0\n",
    "test_data.loc[test_data['Country'].isin(nordic),'Nordic'] = 1\n",
    "test_data['Orthodox'] = 0\n",
    "test_data.loc[test_data['Country'].isin(orthodox),'Orthodox'] = 1\n",
    "test_data['Mediterranean'] = 0\n",
    "test_data.loc[test_data['Country'].isin(mediterranean),'Mediterranean'] = 1\n",
    "test_data['East Europe'] = 0\n",
    "test_data.loc[test_data['Country'].isin(east_europe),'East Europe'] = 1\n",
    "test_data['British Isles'] = 0\n",
    "test_data.loc[test_data['Country'].isin(british_isles),'British Isles'] = 1\n",
    "test_data['German Speaking'] = 0\n",
    "test_data.loc[test_data['Country'].isin(german_speaking),'German Speaking'] = 1\n",
    "test_data['French Speaking'] = 0\n",
    "test_data.loc[test_data['Country'].isin(french_speaking),'French Speaking'] = 1\n",
    "test_data['Northwest Europe'] = 0\n",
    "test_data.loc[test_data['Country'].isin(north_west_europe),'Northwest Europe'] = 1\n",
    "test_data['Romance Language'] = 0\n",
    "test_data.loc[test_data['Country'].isin(romance_language),'Romance Language'] = 1\n",
    "test_data['Germanic Language'] = 0\n",
    "test_data.loc[test_data['Country'].isin(germanic_language),'Germanic Language'] = 1\n",
    "test_data['Slavic Language'] = 0\n",
    "test_data.loc[test_data['Country'].isin(slavic_language),'Slavic Language'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_train_pd = pd.get_dummies(data['Country'])\n",
    "countries_test_pd = pd.get_dummies(test_data['Country'])\n",
    "country_col_to_add = np.setdiff1d(countries_train_pd.columns, countries_test_pd.columns)\n",
    "# add these columns to test, setting them equal to zero\n",
    "for c in country_col_to_add:\n",
    "    countries_test_pd[c] = 0\n",
    "# select and reorder the test columns using the train columns\n",
    "countries_test_pd = countries_test_pd[countries_train_pd.columns]\n",
    "countries_test = countries_test_pd.as_matrix()\n",
    "countries = countries_train_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages_train_pd = pd.get_dummies(data['Language'])\n",
    "languages_test_pd = pd.get_dummies(test_data['Language'])\n",
    "languages_col_to_add = np.setdiff1d(languages_train_pd.columns,languages_test_pd.columns)\n",
    "for c in languages_col_to_add:\n",
    "    languages_test_pd[c] = 0\n",
    "languages_test_pd = languages_test_pd[languages_train_pd.columns]\n",
    "languages_test = languages_test_pd.as_matrix()\n",
    "languages = languages_train_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = np.hstack([data[features].as_matrix(),countries,languages])\n",
    "test_total = np.hstack([test_data[features].as_matrix(),countries_test,languages_test])\n",
    "target = data[target_features].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_sets = 10\n",
    "input_data_cross = np.array_split(total,cross_sets)\n",
    "target_data_cross = np.array_split(target,cross_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_estimators = 100\n",
    "max_features = 10\n",
    "learning_rate = 0.11\n",
    "max_depth = 4\n",
    "subsample = 1\n",
    "\n",
    "for nr_estimators in range(10,200,10):\n",
    "    mse_boost = 0\n",
    "    for i in range(cross_sets):\n",
    "        train_input_data = np.concatenate([input_data_cross[l] for l in range(len(input_data_cross)) if l!=i])\n",
    "        train_target_data = np.concatenate([target_data_cross[l] for l in range(len(target_data_cross)) if l!=i])\n",
    "        validation_input_data = input_data_cross[i]\n",
    "        validation_target_data = target_data_cross[i]\n",
    "        boost = GradientBoostingRegressor(n_estimators=nr_estimators,verbose=0,max_features= max_features,learning_rate=learning_rate,max_depth=max_depth,subsample=subsample)\n",
    "        boost = boost.fit(train_input_data,np.ravel(train_target_data))\n",
    "\n",
    "        predictions_boost = boost.predict(validation_input_data)\n",
    "        mse_boost += np.dot(predictions_boost - np.ravel(validation_target_data),predictions_boost - np.ravel(validation_target_data))\n",
    "\n",
    "    print(subsample,learning_rate,max_features,nr_estimators,mse_boost/float(data['Score'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuned_boost = GradientBoostingRegressor(n_estimators=100,verbose=1,max_features = 10,learning_rate=0.11,max_depth=4)\n",
    "tuned_boost = tuned_boost.fit(total,np.ravel(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(test_data['Country'],tuned_boost.predict(test_total)),key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = (tuned_boost.predict(test_total)+points_per_country/(test_data['Countries competing'].apply(float)))*test_data['Countries Voting']*2\n",
    "print sum(predictions)\n",
    "print points_per_country * test_data['Countries Voting'][0] * 2\n",
    "pred_outcome = sorted(zip(test_data['Country'],predictions),key = lambda x: x[1], reverse=True)\n",
    "for l in pred_outcome:\n",
    "    print l[0], int(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  np.hstack([data[features].as_matrix(),countries,languages])\n",
    "total_pd = pd.concat([data[features],countries_train_pd,languages_train_pd],axis=1)\n",
    "data_pd = pd.concat([total_pd,data['Year'],data['Normalized Score 2'],data['Normalized Score']],axis=1)\n",
    "data_2014 = data_pd[data_pd['Year']!=2015].drop(['Year','Normalized Score 2','Normalized Score'],axis = 1)\n",
    "target_2014_pd = data_pd[data_pd['Year']!= 2015]['Normalized Score']\n",
    "target_2014 = data_pd[data_pd['Year']!= 2015]['Normalized Score'].as_matrix()\n",
    "data_2015 = data_pd[data_pd['Year']==2015].drop(['Year','Normalized Score','Normalized Score 2'],axis = 1)\n",
    "country_data_2015 = data[data['Year']==2015][['Country','Countries competing','Countries Voting','Final']]\n",
    "data_final_2015 = data_2015[country_data_2015['Final']==1]\n",
    "country_data_final_2015 = country_data_2015[country_data_2015['Final']==1]\n",
    "target_2015 = data_pd[data_pd['Year']==2015]['Normalized Score 2']\n",
    "target_final_2015 = (target_2015[country_data_2015['Final']==1] +points_per_country/(country_data_final_2015['Countries competing'].apply(float)))*country_data_final_2015['Countries Voting']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuned_boost_2014 = GradientBoostingRegressor(n_estimators=80,verbose=1,max_features = 9,learning_rate=0.11,max_depth=4)\n",
    "tuned_boost_2014 = tuned_boost.fit(data_2014,np.ravel(target_2014))\n",
    "predict_2015 = np.round(tuned_boost.predict(data_final_2015)*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_2015 = data[(data['Year']==2015) & data['Final']==1]['Country'].to_frame(name='Country')\n",
    "print country_data_final_2015['Countries Voting'].mean()\n",
    "#print data_2014['Germany']\n",
    "mse_2015 = 0\n",
    "mse_a_2015 = 0\n",
    "countries_2015['avg'] = 0\n",
    "for c in range(len(countries_2015.values)):\n",
    "    name = countries_2015.values[c][0]\n",
    "    if name == \"Australia\":\n",
    "        avg = 106\n",
    "    else:\n",
    "        avg = np.round(target_2014_pd[data_2014[name]==1].mean()*40)\n",
    "    mse_2015 += int(target_final_2015[c+797]-predict_2015[c])**2\n",
    "    mse_a_2015 += int(target_final_2015[c+797]-avg)**2\n",
    "    print name,int(target_final_2015[c+797]),int(predict_2015[c]),int(avg),int(target_final_2015[c+797]-predict_2015[c])**2,int(target_final_2015[c+797]-avg)**2\n",
    "print mse_2015\n",
    "print mse_a_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuned_boost_2014 = GradientBoostingRegressor(n_estimators=100,verbose=1,max_features = 9,learning_rate=0.11,max_depth=4)\n",
    "tuned_boost_2014 = tuned_boost.fit(data_2014,np.ravel(target_2014))\n",
    "predictions_2015 = (tuned_boost.predict(data_final_2015.as_matrix())+points_per_country/(country_data_final_2015['Countries competing'].apply(float)))*country_data_final_2015['Countries Voting']\n",
    "\n",
    "print sorted(zip(country_data_final_2015['Country'],target_final_2015,predictions_2015),key = lambda x: x[1], reverse=True)\n",
    "print sum(predictions_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(list(total_pd.columns.values),tuned_boost.feature_importances_),key = lambda x: x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "videocounts = data[(data['Videocounts']!=0)]['Normalized YT count']\n",
    "score = data[(data['Videocounts']!=0)]['Normalized Score 2']\n",
    "plt.scatter(videocounts,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guess = [x*scipy.special.binom(10,x)*scipy.special.binom(8,10-x) for x in range(10,1,-1)]\n",
    "guess2 = [scipy.special.binom(10,x)*scipy.special.binom(8,10-x) for x in range(10,1,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.sum(guess)/scipy.special.binom(18,10)\n",
    "print np.sum(guess2[:1])/scipy.special.binom(18,10)\n",
    "print np.sum(guess2[:2])/scipy.special.binom(18,10)\n",
    "print np.sum(guess2[:3])/scipy.special.binom(18,10)\n",
    "print np.sum(guess2[:4])/scipy.special.binom(18,10)\n",
    "print np.sum(guess2[:5])/scipy.special.binom(18,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_small = np.hstack([data[features].as_matrix(),countries,languages])\n",
    "test_total_small = np.hstack([test_data[features].as_matrix(),countries_test,languages_test])\n",
    "target = data[target_features].as_matrix()\n",
    "\n",
    "cross_sets = 10\n",
    "input_data_small_cross = np.array_split(total_small,cross_sets)\n",
    "target_data_small_cross = np.array_split(target,cross_sets)\n",
    "\n",
    "nr_estimators = 80\n",
    "max_features = 9\n",
    "learning_rate = 0.11\n",
    "max_depth = 4\n",
    "\n",
    "for max_features in range(1,20,2):\n",
    "    mse_boost = 0\n",
    "    for i in range(cross_sets):\n",
    "        train_input_small_data = np.concatenate([input_data_small_cross[l] for l in range(len(input_data_cross)) if l!=i])\n",
    "        train_target_small_data = np.concatenate([target_data_small_cross[l] for l in range(len(target_data_cross)) if l!=i])\n",
    "        validation_input_data = input_data_small_cross[i]\n",
    "        validation_target_data = target_data_small_cross[i]\n",
    "        boost = GradientBoostingRegressor(n_estimators=nr_estimators,verbose=0,max_features= max_features,learning_rate=learning_rate,max_depth=max_depth,subsample=subsample)\n",
    "        boost = boost.fit(train_input_small_data,np.ravel(train_target_small_data))\n",
    "\n",
    "        predictions_small = boost.predict(validation_input_data)\n",
    "        mse_boost += np.dot(predictions_small - np.ravel(validation_target_data),predictions_small - np.ravel(validation_target_data))\n",
    "\n",
    "    print(subsample,learning_rate,max_features,nr_estimators,mse_boost/float(data['Score'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print data.columns.values\n",
    "data_set = data[(data['Language']=='English') ]['Normalized Score']*24\n",
    "print i, data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Language']!='English') ]['Normalized Score']*24\n",
    "print i, data_set.mean(),  data_set.std(), data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print data.columns.values\n",
    "data_set = data[(data['Experienced artist']==1) ]['Normalized Score']*24\n",
    "print i, data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Experienced artist']==0) ]['Normalized Score']*24\n",
    "print i, data_set.mean(),  data_set.std(), data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print data.columns.values\n",
    "data_set = data[(data['Scandinavia']==1) ]['Normalized Score']*24\n",
    "print data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Scandinavia']!=1) ]['Normalized Score']*24\n",
    "print data_set.mean(),  data_set.std(), data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set = data[(data['Male singer']==1) & (data['Experienced artist']==1)]['Normalized Score']*24\n",
    "print data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Male singer']==1) & (data['Experienced artist']==0)]['Normalized Score']*24\n",
    "print data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Male singer']==0) & (data['Experienced artist']==1)]['Normalized Score']*24\n",
    "print data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Male singer']==0) & (data['Experienced artist']==0)]['Normalized Score']*24\n",
    "print data_set.mean(),  data_set.std(), data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set = data[(data['Country']=='Russia') ]['Normalized Score']*24\n",
    "print i, data_set.mean(),  data_set.std(), data_set.count()\n",
    "data_set = data[(data['Country']!='Russia') ]['Normalized Score']*24\n",
    "print i, data_set.mean(),  data_set.std(), data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.sort_values(['Normalized YT count'],ascending=False)[['Year','Country','Artist','Song','Normalized YT count','Videocounts','Normalized Score 2']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://blog.datadive.net/prediction-intervals-for-random-forests/\n",
    "def pred_ints(model, X, percentile=95):\n",
    "    err_down = []\n",
    "    err_up = []\n",
    "    for x in range(len(X)):\n",
    "        preds = []\n",
    "        for pred in model.estimators_:\n",
    "            preds.append(pred[0].predict(np.reshape(X[x],(1,len(X[x]))))[0])\n",
    "        err_down.append(np.percentile(preds, (100 - percentile) / 2. ))\n",
    "        err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2.))\n",
    "    return err_down, err_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_lower = np.array(pred_ints(tuned_boost,test_total,percentile=95))*21*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print upper_lower\n",
    "#print np.array(predictions)\n",
    "#print np.array(test_data['Country'])\n",
    "error_means = np.vstack([np.array(test_data['Country']),np.array(predictions),upper_lower])\n",
    "#print error_means\n",
    "#a[a[:,1].argsort()]\n",
    "sorted_error_means = error_means[:,error_means[1].argsort()[::-1]]\n",
    "#print sorted_error_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig = plt.subplots()\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(20,15))\n",
    "ind = np.arange(sorted_error_means.shape[1])\n",
    "fig = plt.bar(ind, sorted_error_means[1],color='g', yerr = np.abs(sorted_error_means[2:]),ecolor='black',alpha=0.8)\n",
    "plt.xticks(ind+0.4,sorted_error_means[0],rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.sort_values(['Normalized YT count'],ascending=False)[['Year','Country','Artist','Song','Normalized YT count','Videocounts','Normalized Score 2']][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
